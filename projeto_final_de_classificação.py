# -*- coding: utf-8 -*-
"""Projeto Final de Classificação.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gs3WurHUZI93JLlwc8ZM9Rf3dmrOvHQv

# **Classificação de clientes inadimplentes - Segmento Comunicação Digital**
Classificação com o uso de machinne learning em um banco de dados real.

##**1. Introdução**##
  Os clientes inadimplentes talvez seja uma das principais dores sofridas por muitas empresas atualmente e as razões, por esssa inadimplência, podem estar relacionados com diversos fatores, como: insatisfação com o que foi ofertado, problemas financeiros, etc. Seja qual for o motivo que tenha levado à inadimplência do cliente, é fundamental entender esses motivos e utilizar técnicas de cobrança para contornar essa situação. Afinal esse mesmo cliente que já foi inadimplente pode ser um consumidor de extrema importância para o  negócio no futuro. Pois, saber como lidar bem com clientes inadimplentes e resolver a situação de uma maneira adequada mostrará a qualidade do  atendimento da empresa, que pode ajudar a fidelizar consumidores e até mesmo gerar defensores da marca.
  Entretanto sabemos que nem sempre é fácil negociar divídas e muitas vezes o tempo e energia são "desperdiçados" com os clientes "errados". O primeiro passo para realizar técnicas de cobranças é identificar os clientes com maiores chances de negociação. Empresas que têm a cultura data driven otimizam essa busca, pois maximizam seus resultados  utilizando os dados para extrair informações e usam as informações para tomar decisões.
  Em um cenário de comunicação digital, dentre o portfólio de produtos, como SMS, E-mail, Telecom, Bots e Agentes Virtuais de Voz, a implementação, através da ciência de dados e machinne learning, de uma solução para o produto de agentes virtuais de voz, que a possibilidade de classificar clientes inadimplentes propensos ou não a um acordo comercial – pagamento da dívida.  
  Através da implementação da solução, espera-se agregar valor ao produto e as partes envolvidas, otimizar o tempo nas tomadas de decisões, fornecer  insights aos analistas de negócios, automatizar processos, escalar a operação, possibilitar ofertas direcionadas / diferenciadas de acordos comerciais – conforme o público alvo, aumentar os acordos comerciais, reduzir os custos operacionais e consequentemente o aumento de receita.
  Com base no contexto, problema e impacto esperado desta solução, entendemos que a estratégia será a construção de um modelo de classificação, utilizando os dados históricos da companhia em junção com um parceiro de negócios, para identificação da propensão de cada cliente no pagamento da divida. Para inicio, pretendemos utilizar algumas features como sexo, score, idade, classe econômica, situação empregatícia, persona de crédito, unidade federativa e renda, afim de identificar as suas respectivas correlações com a variavel resposta - acordo.
  A taxa de acordo entre empresas e devedores varia de acordo com a estratégia adotada e com mais informaões dá pra planejar mais estratégias. Não fechar um acordo não significa apenas deixar de ganhar o valor de contrato, mas também todas as outras eventuais oportunidades de negócios.

Mas quais são as características que leva um cliente a não fechar um acordo?

#2.**Coleta de dados**
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd 

dados = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/TERA/Desafio Final/base_projeto.csv', sep = ';', encoding = 'UTF-8') 
dados.sample(10)

"""**DICIONÁRIO**
- SEXO: Sexo do cliente
- IDADE: Idade do cliente
- GERAÇÃO: Geração que o cliente nasceu
- RENDA: Renda do cliente	
- CLASSEECONOMICA: A classe econômica a qual o cliente está inserido	
- UF: Estaddo de residência do ciente
- AREARISCO: Variável que indica se a área em que o cliente mora é boa ou má pagadora
- FLAG_DE_SOCIO: se é sócio de algum negócio
- FLAG_DE_VEICULO: se possui veículo
- FLAG_DE_BOLSAFAMILIA: Se recebe o bolsa família	
- FLAG_DE_APOSENTADOS: Se já aposentou
- FLAG_DE_OBITO:se  já faleceu	
- FLAG_DE_IMOVEL: se possui imóvel	
- EMPREGO_FORMAL: Se é empregado clt	
- SCORE: Score  Serasa 	
-	SCORE_FAIXA:  	
- PERSONADECREDITO: 	
- SCORE_DIGITAL: 	
- ACORDO: Target se o acordo foi ou não realizado

## EDA
"""

dados.info()

"""O data set possui poucos valores nulos."""

dados.describe()

"""Os clientes desse banco têm uma idade média de 44 anos. Idade mínima de 18 anos e máxima de 97 anos. A pontuação de crédito fica em torno de 437 pontos e o índice médio de estabilidade de emprego é 0,5.
Analisando a variável de Acordo, observa-se que 50% dos clientes fecharam acordo.

# **3. TRATAMENTO DE DADOS**
"""

db_tratamento = dados.copy()

# Dados numéricos são mais fáceis de trabalhar. Sendo assim a transformação de nomes por números pode facilitar a exploração dos dados
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
db_tratamento['RENDA'] = le.fit_transform(db_tratamento['RENDA'])
db_tratamento['GERAÇÃO'] = le.fit_transform(db_tratamento['GERAÇÃO'])
db_tratamento['SCORE_FAIXA'] = le.fit_transform(db_tratamento['SCORE_FAIXA'])
db_tratamento['PERSONADECREDITO'] = le.fit_transform(db_tratamento['PERSONADECREDITO'])
db_tratamento['SCORE_DIGITAL'] = le.fit_transform(db_tratamento['SCORE_DIGITAL'])
db_tratamento['SEXO'] = le.fit_transform(db_tratamento['SEXO'])
db_tratamento['UF'] = le.fit_transform(db_tratamento['UF'])
db_tratamento['CLASSEECONOMICA'] = le.fit_transform(db_tratamento['CLASSEECONOMICA'])
db_tratamento.head(10)

db_tratamento.hist(figsize=(12,10));

"""- A variável Idade possui a maioria dos clientes entre 25 a 50 anos e poucos clientes têm idade superior a 75 anos.
- A  distribuição da variável geração, mostra um grande número de clientes que são da geração y e z.
- A distribuição da variável renda do Cliente mostra que a maioria ganha ate 2000,00 por mês.
- Os graficos das variáveis área de risco, flag de imovel, veiculo, obito e aposentados indicam que a maioria dos clientes não possuem casa própria e veiculo. A maioria conrinua viva e ainda não aposentaram e não moram em área de risco. 
- A empregabilidade está balanceada.
- A distribuição da variável score está levemente assimétrica a direita.
- E, para a variável Acordo, observa-se balanceamento.
"""

db_tratamento[["RENDA"]].value_counts(sort = False)
#
#0 = 1000 a 2000   
#1 = 2001 a 3000     
#2 = 3001 a 4000     
#3 = 4001 a 5000    
#4 = 5001 a 6000    
#5 = 6001 a 7000     
#6 = 7001 a 8000      
#7 = 8001 a 9000           
#8 = 9001 a 10000           
#9 = 10001 a 11000         
#10 = 11001 a 12000          
#11 = 12001 a 13000         
#12 = 13001 a 14000        
#13 = 14001 a 15000          
#14 = 15001 a 16000          
#15 = 16001 a 17000          
#16 = 17001 a 18000         
#17 = 18001 a 19000         
#18 = 19001 a 20000         
#19 = 20001 a 21000       
#20 = 21001 a 22000       
#21 = 22001 a 23000      
#22 = 23001 a 24000         
#23 = 24001 a 25000        
#24 = 25001 a 26000        
#25 = 26001 a 27000         
#26 = 27001 a 28000       
#27 = 28001 a 29000      
#28 = 29001 a 30000

db_tratamento.info()

db_tratamento.dropna(inplace=True) #Eliminação dos dados nulos, visto que são poucos.

"""## Visualização de Dados - Qual o perfil de clientes que fecharam o acordo?"""

import numpy as np #  pacote algebra linear;
import seaborn as sns #visualização de dados;
import matplotlib.pyplot as plt

"""**Variáveis Numéricas**"""

myred='#CD5C5C'
myblue='#6495ED'
mygreen='#90EE90'
cols= [myred, myblue,mygreen]

# porcentagem de clientes que sairam do banco
db_tratamento['ACORDO'].sum()/db_tratamento['ACORDO'].count()

db_tratamento['ACORDO_cat']=db_tratamento['ACORDO'].replace([0,1],['Não','Sim'])
x_cont=['SCORE','IDADE']
fig, ax = plt.subplots(1, 2, figsize=(22, 6));
db_tratamento[db_tratamento.ACORDO_cat == "Não"][x_cont].hist( bins=30, color="blue", alpha=0.5, ax=ax);
db_tratamento[db_tratamento.ACORDO_cat == "Sim"][x_cont].hist( bins=30, color="red", alpha=0.5, ax=ax);

sns.boxplot(x="ACORDO_cat", y="IDADE", data=db_tratamento)

sns.boxplot(x="ACORDO_cat", y="SCORE", data=db_tratamento)

"""Quanto maior o score, maior a chance de fechamento de acordo
Quanto mais velho, maior a chance de fechamento no acordo

###**Variáveis Categóricas**

 Primeiro vamos analisar as variáveis categóricas em relação a variável de saída (Acordo). Utilizaremos gráficos de barras para esse fim.
"""

db_tratamento

sns.countplot(data=dados,x='ACORDO',palette=cols);

"""O gráfico de barras informa grande equilíbrio de "Acordo" e "Não acordo"."""

dadosCopia = db_tratamento.copy

dados_plot = db_tratamento.groupby('SEXO').ACORDO.mean().reset_index() 
dados_plot

#Sexo em relação a variável churn
x =dados_plot['SEXO']
y=dados_plot['ACORDO']
plt.bar(x,y,color=cols);
plt.title('Taxa de Acordo em relação ao Sexo');
plt.xlabel('Sexo');
plt.ylabel('Taxa de Acordo');

"""Sexo parece não influenciar muito no acordo."""

dado_plot = db_tratamento.groupby('GERAÇÃO').ACORDO.mean().reset_index()
dado_plot

x =dado_plot['GERAÇÃO']
y=dado_plot['ACORDO']
plt.bar(x,y);
plt.title('Taxa de Acordo em relação a GERAÇÃO');
plt.xlabel('GERAÇÃO');
plt.ylabel('Taxa de Acordo');

"""baby boomer e geração x parece ser mais propensos a fechar acordo"""

dado_plot = db_tratamento.groupby('EMPREGO_FORMAL').ACORDO.mean().reset_index()
dado_plot

"""Ter emprego não infuencia muito no acordo."""

dado_plot = db_tratamento.groupby('SCORE_DIGITAL').ACORDO.mean().reset_index()
dado_plot

"""score digital bem balanceado"""

x =dado_plot['SCORE_DIGITAL']
y=dado_plot['ACORDO']
plt.bar(x,y);
plt.title('Taxa de Acordo em relação a SCORE_DIGITAL');
plt.xlabel('SCORE_DIGITAL');
plt.ylabel('Taxa de Acordo');

dado_plot = db_tratamento.groupby('RENDA').ACORDO.mean().reset_index()
dado_plot

x =dado_plot['RENDA']
y=dado_plot['ACORDO']
plt.bar(x,y);
plt.title('Taxa de Acordo em relação a SCORE_DIGITAL');
plt.xlabel('SCORE_DIGITAL');
plt.ylabel('Taxa de Acordo');

dado_plot = db_tratamento.groupby('FLAG_DE_IMOVEL').ACORDO.mean().reset_index()
dado_plot

dado_plot = db_tratamento.groupby('FLAG_DE_VEICULO').ACORDO.mean().reset_index()
dado_plot

db_tratamento[["FLAG_DE_VEICULO"]].value_counts(sort = False)

dado_plot = db_tratamento.groupby('PERSONADECREDITO').ACORDO.mean().reset_index()
dado_plot

"""Analisando o histórico de clientes, observamos algumas características entre os clientes que fecharam o acordo

Observa-se que clientes mais velhos, com maior score no serasa e que possuem veículo estão mais propensos a fechar o acordo

Uma atenção especial na renda. Renda altas acima de 30 mil estão com taxa de acordo menor do que rendas mais baixas.

##**5. PRÉ PROCESSAMENTO**

Alguns algoritmos de machine learning não aceitam variáveis categóricas e por isso precisamos utilizar técnicas para converter as classes em números.

Ou seja, convertemos as colunas categóricas em numéricas simplesmente atribuindo números inteiros a classes distintas.

Para as variáveis categóricas com apenas duas classes utilizaremos a função Label Encoder para converte colunas categóricas em numéricas simplesmente atribuindo números inteiros a valores distintos.

Por exemplo, a coluna sexo tem dois valores: Feminino e Masculino . Após aplicar a função, os valores serão transformados em 1 e 0.
"""

df1 = db_tratamento.drop(columns = ['ACORDO_cat'])
df1

DF2 = df1[['IDADE', 'GERAÇÃO', 'RENDA', 'SCORE','PERSONADECREDITO','ACORDO']]

df1 = df1.dropna()
DF2 = DF2.dropna()

"""## **6.MODELOS DE CLASSIFICAÇÃO**"""

y = DF2['ACORDO']
X = df1.drop('ACORDO',axis = 1)
X

features = ['SEXO',	'IDADE',	'GERAÇÃO','RENDA','CLASSEECONOMICA','UF',
            'AREARISCO',	'FLAG_DE_SOCIO',	'FLAG_DE_VEICULO',	'FLAG_DE_BOLSAFAMILIA',	'FLAG_DE_APOSENTADOS',
            'FLAG_DE_OBITO',	'FLAG_DE_IMOVEL',	'EMPREGO_FORMAL',	'SCORE',	'SCORE_FAIXA',	'PERSONADECREDITO','SCORE_DIGITAL']

#Variáveis Continuas
x_cont=['IDADE','SCORE']
x_cont

"""As variáveis numéricas IDADE E Score estão em diferentes escalas e isso pode causar problemas no treinamento dos modelos de machine learning.

Para resolver esse problemas utilizaremos o método MiniMax para padronizar as variáveis contínuas, ou seja, vamos deixar todas as variáveis contínuas numa escala entre 0 e 1.
"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X[x_cont] = scaler.fit_transform(X[x_cont])
X[x_cont]

# Separando em dados teste e treinamento 
from sklearn.model_selection import train_test_split 

x1_treino, x1_teste, y1_treino, y1_teste  = train_test_split(X, y, test_size = 0.3,random_state = 1)
x1_treino

"""## Aplicando o modelo de regressão logística"""

from sklearn.linear_model import LogisticRegression

modelo = LogisticRegression()
modelo.fit(x1_treino,y1_treino)

#realizando classificação na amostra de teste
y1_previsto = modelo.predict(x1_teste)

#criando a matriz de confusão
from sklearn.metrics import confusion_matrix

cm=confusion_matrix(y1_teste,y1_previsto)
cm

sns.heatmap(cm,annot=True, fmt="d")
plt.xlabel('Predito')
plt.ylabel('Real')

#calculando as metricas do modelo
from sklearn.metrics import classification_report,f1_score,precision_score,average_precision_score,recall_score,accuracy_score,roc_auc_score

#relatório de classificação
cr = classification_report(y1_teste,y1_previsto,labels=[0,1])
print(cr)

acc1 = accuracy_score(y1_previsto, y1_teste)
sen1 = recall_score(y1_previsto, y1_teste)
pre1 = precision_score(y1_previsto, y1_teste)
f1s1 = f1_score(y1_previsto, y1_teste)
auc1 = roc_auc_score(y1_previsto, y1_teste)
medidas = {'Acurácia': [acc1], 'Sensibilidade': [sen1], 'Precisão': [pre1], 'F1-Score': [f1s1], 'AUC': [auc1]}

DF1 = pd.DataFrame(data = medidas, index = ['Regressão_Logistica1']).round(4)
DF1

!pip install eli5
import eli5
from eli5.sklearn import PermutationImportance

# Identificando as melhores features
perm = PermutationImportance(modelo, random_state=1).fit(x1_teste, y1_teste)
eli5.show_weights(perm, feature_names = features)

"""##TREINAMENTO COM AS FEATURES MAIS IMPORTANTES"""

top10_features = ['IDADE',	'GERAÇÃO','RENDA',
            	'FLAG_DE_SOCIO',	'FLAG_DE_VEICULO','FLAG_DE_APOSENTADOS',
            'FLAG_DE_OBITO','SCORE',	'SCORE_FAIXA',	'PERSONADECREDITO']
X2 = df1[top10_features]

x2_treino, x2_teste, y2_treino, y2_teste  = train_test_split(X2, y, test_size = 0.3,random_state = 1)
x2_treino

modelo2 = LogisticRegression()
modelo2.fit(x2_treino,y2_treino)

#realizando classificação na amostra de teste 2
y_previsto2 = modelo2.predict(x2_teste)

cm=confusion_matrix(y2_teste,y_previsto2)
cm

cr = classification_report(y2_teste,y_previsto2,labels=[0,1])
print(cr)

acc2 = accuracy_score(y2_teste,y_previsto2)
sen2 = recall_score(y2_teste,y_previsto2)
pre2 = precision_score(y2_teste,y_previsto2)
f1s2 = f1_score(y2_teste,y_previsto2)
auc2 = roc_auc_score(y2_teste,y_previsto2)
medidas2 = {'Acurácia': [acc2], 'Sensibilidade': [sen2], 'Precisão': [pre2], 'F1-Score': [f1s2], 'AUC': [auc2]}

Df2 = pd.DataFrame(data = medidas2, index = ['Regressão_Logistica2']).round(4)
Df2

"""Com as variáveis mais importantes, conseguimos aumentar a sensibilidade, mas a precisão diminuiu.

## Aplicando o modelo de decision tree
"""

from sklearn.tree import DecisionTreeClassifier
x3_treino, x3_teste, y3_treino, y3_teste  = train_test_split(X, y, test_size = 0.3,random_state = 1)
x3_treino
modelotree = DecisionTreeClassifier() 
modelotree.fit(x3_treino,y3_treino)

y_pred_tree = modelotree.predict(x3_teste)

y_pred_tree

y3_teste

from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(modelotree, x3_teste, y3_teste, cmap='Blues')

cr = classification_report(y3_teste,y_pred_tree,labels=[0,1])
print(cr)

acc3 = accuracy_score(y_pred_tree, y3_teste)
sen3 = recall_score(y_pred_tree, y3_teste)
pre3 = precision_score(y_pred_tree, y3_teste)
f1s3 = f1_score(y_pred_tree, y3_teste)
auc3 = roc_auc_score(y_pred_tree, y3_teste)
medidas3 = {'Acurácia': [acc3], 'Sensibilidade': [sen3], 'Precisão': [pre3], 'F1-Score': [f1s3], 'AUC': [auc3]}
Df3 = pd.DataFrame(data = medidas3, index = ['Árvore de Decisão']).round(4)
Df3

perm = PermutationImportance(modelotree, random_state=1).fit(x3_teste, y3_teste)
eli5.show_weights(perm, feature_names = features)

"""## TREINAMENTO COM AS FEATURES MAIS IMPORTANTES"""

top12_features = ['UF', 'IDADE', 'GERAÇÃO', 'SCORE', 'SCORE_DIGITAL', 'FLAG_DE_VEICULO', 'RENDA', 
                  'FLAG_DE_OBITO', 'CLASSEECONOMICA', 'PERSONADECREDITO', 'FLAG_DE_BOLSAFAMILIA', 'SCORE_FAIXA']
X3 = df1[top12_features]

x4_treino, x4_teste, y4_treino, y4_teste  = train_test_split(X3, y, test_size = 0.3,random_state = 1)
x4_treino
modelotree2 = DecisionTreeClassifier() 
modelotree2.fit(x4_treino,y4_treino)

y_pred_tree2 = modelotree2.predict(x4_teste)

plot_confusion_matrix(modelotree2, x4_teste, y4_teste, cmap='Blues')

cr = classification_report(y4_teste, y_pred_tree2, labels=[0,1])
print(cr)

acc4 = accuracy_score(y_pred_tree2, y4_teste)
sen4 = recall_score(y_pred_tree2, y4_teste)
pre4 = precision_score(y_pred_tree2, y4_teste)
f1s4 = f1_score(y_pred_tree2, y4_teste)
auc4 = roc_auc_score(y_pred_tree2, y4_teste)
medidas4 = {'Acurácia': [acc4], 'Sensibilidade': [sen4], 'Precisão': [pre4], 'F1-Score': [f1s4], 'AUC': [auc4]}
Df4 = pd.DataFrame(data = medidas4, index = ['Árvore de Decisão Top 12']).round(4)
Df4

"""Considerando as features mais importantes, o modelo chega a 67% de acurácia. Com uma bom equilibrio de especificidade e sensibilidade.

## **6. AJUSTE**

O ajuste será realizado com o GridSearch considerando o melhor modelo até agora( DecisionTree) e as melhores 12 features.
"""

from sklearn.model_selection import GridSearchCV


dt = DecisionTreeClassifier(random_state=61658)

# dicionário de hiperparâmetros a serem testados
params = {
    'criterion' : ['gini','entropy'],
    'max_depth' : [2,3,4,5,6,7,8,9],
}

grid = GridSearchCV(
    dt,
    params,
    cv=10,
    scoring='roc_auc',
    verbose=10,
    n_jobs=1,
)

grid.fit(x4_treino,y4_treino)

grid.best_params_  # Melhores parâmetros encontrados

grid.best_score_ # Melhor pontuação de ajuste

grid.best_estimator_ # melhores estimadores

modelo_ajustado = DecisionTreeClassifier(criterion='gini', max_depth=7, random_state=61658) 
modelo_ajustado.fit(x4_treino,y4_treino)

y_pred_treeAjustado = modelo_ajustado.predict(x4_teste)

print (pd.crosstab(y4_teste, y_pred_treeAjustado, rownames = ['Real'], colnames = ['Predito'], margins = True))

from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(modelo_ajustado, x4_teste, y4_teste, cmap='Blues')

print(classification_report(y4_teste, y_pred_treeAjustado))

acc5 = accuracy_score(y_pred_treeAjustado, y4_teste)
sen5 = recall_score(y_pred_treeAjustado, y4_teste)
pre5 = precision_score(y_pred_treeAjustado, y4_teste)
f1s5 = f1_score(y_pred_treeAjustado, y4_teste)
auc5 = roc_auc_score(y_pred_treeAjustado, y4_teste)
medidas5 = {'Acurácia': [acc5], 'Sensibilidade': [sen5], 'Precisão': [pre5], 'F1-Score': [f1s5], 'AUC': [auc5]}
Df5 = pd.DataFrame(data = medidas5, index = ['Árvore de Decisão top 12 Ajustada']).round(4)
Df5

"""Após o ajuste do modelo, todas as métricas melhoraram. Chegando a quase 70% de acurácia"""

from sklearn.metrics import roc_auc_score
roc_auc_score(y4_teste, grid.predict_proba(x4_teste)[:,1])

"""## **Aplicando o modelo de XGBoost**"""

from numpy import loadtxt
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

# ajuste do modelo nos dados de treino
xgb = XGBClassifier(learning_rate =0.1,
 n_estimators=1000,
 max_depth=6,
 min_child_weight=1,
 gamma=0,
 subsample=0.8,
 colsample_bytree=0.8,
 objective= 'binary:logistic',
 nthread=4,
 scale_pos_weight=1.0,
 seed=27)
xgb.fit(x4_treino, y4_treino)

preditos_xgb = xgb.predict(x4_teste)
print (pd.crosstab(y4_teste,preditos_xgb, rownames=['Real'], colnames=['Predito'], margins=True))

from sklearn.metrics import classification_report,confusion_matrix
print(classification_report(y4_teste, preditos_xgb))

acc6 = accuracy_score(preditos_xgb, y4_teste)
sen6 = recall_score(preditos_xgb, y4_teste)
pre6 = precision_score(preditos_xgb, y4_teste)
f1s6 = f1_score(preditos_xgb, y4_teste)
auc6 = roc_auc_score(preditos_xgb, y4_teste)
medidas6 = {'Acurácia': [acc6], 'Sensibilidade': [sen6], 'Precisão': [pre6], 'F1-Score': [f1s6], 'AUC': [auc6]}
Df6 = pd.DataFrame(data = medidas6, index = ['XGBClassifier']).round(4)
Df6

from sklearn.ensemble import VotingClassifier # União dos melhores modelos
# Voting Classifier com soft voting 
voto = VotingClassifier(estimators=[('modelo_ajustado', modelo_ajustado),('xgb',xgb)], voting='soft')
voto = voto.fit(x4_treino,y4_treino)
y_predito = voto.predict(x4_teste)
print(classification_report(y4_teste, y_predito))

acc7 = accuracy_score(y_predito, y4_teste)
sen7 = recall_score(y_predito, y4_teste)
pre7 = precision_score(y_predito, y4_teste)
f1s7 = f1_score(y_predito, y4_teste)
auc7 = roc_auc_score(y_predito, y4_teste)
medidas7 = {'Acurácia': [acc7], 'Sensibilidade': [sen7], 'Precisão': [pre7], 'F1-Score': [f1s7], 'AUC': [auc7]}
Df7 = pd.DataFrame(data = medidas7, index = ['VotingClassifier']).round(4)
Df7

pd.concat([DF1, Df2, Df3, Df4, Df5, Df6, Df7])

"""#CONCLUSÃO

Os modelos de ensemble como o voting e o XGB, por serem mais robustos, tiveram um melhor resultado e passaram dos 70% nas principais métricas de avaliação. A desvantagem é que eles são muitos custosos e dificilmente vão para a produção. Por outro lado, modelos mais simples como a árvore de decisão também teve um bom desempenho. Apenas realizando uma seleção das melhores variáveis.
"""